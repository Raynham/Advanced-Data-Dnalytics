{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba36903-ee71-4a62-9bb8-8e0270fdae64",
   "metadata": {},
   "source": [
    "# Advanced Text Analytics Lab 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6465c348-807b-4a0c-89b0-55b9054cf875",
   "metadata": {},
   "source": [
    "This notebook is the first of two lab notebooks that you will submit as part of your assessment for the Advanced Data Analytics unit. \n",
    "\n",
    "This notebook is contains three sections:\n",
    "1. **Word embeddings:** This will introduce you to loading and training word embeddings using the Gensim library.\n",
    "2. **Introducing neural text classifiers:** Here we show you how to construct a neural network text classifier for sentiment analysis using Pytorch. \n",
    "3. **Improving neural text classifiers:** This section gives you a chance to improve the classifier from the previous section by applying what we have learned in the lectures.\n",
    "\n",
    "## Learning Outcomes\n",
    "\n",
    "These sections will contain tutorial-like instructions, as you have seen in previous text analytics labs. On completing these sections, the intended learning outcomes are that you will be able to...\n",
    "1. Load pretrained word embeddings models.\n",
    "1. Learn word embeddings from an unlabelled dataset.\n",
    "1. Recognise the steps required to train and test a neural text classifier with Pytorch\n",
    "1. Adapt the architecture of a neural text classifier.\n",
    "\n",
    "## Your Tasks\n",
    "\n",
    "Inside each of these sections there are several **'To-do's**, which you must complete for your summative assessment. Your marks will be based on your answers to these to-dos. Please make sure to:\n",
    "1. Include the output of your code in the saved notebook. Plots and printed output should be visible without re-running the code. \n",
    "1. Include all code needed to generate your answers.\n",
    "1. Provide sufficient comments to understand how your method works.\n",
    "1. Write text in a cell in markdown format where a written answer is required. You can convert a cell to markdown format by pressing Escape-M. \n",
    "\n",
    "There are also some unmarked 'to-do's that are part of the tutorial to help you learn how to implement and use the methods studied here.\n",
    "\n",
    "## Marking Criteria\n",
    "\n",
    "1. The coursework (both notebooks) is worth 30% of the unit in total. \n",
    "1. There is a total of 100 marks available for both lab notebooks. \n",
    "1. This notebook is worth 66 of those marks.\n",
    "1. The number of marks for each to-do out of 100 is shown alongside each to-do.\n",
    "1. For to-dos that require you to write code, a good solution would meet the following criteria (in order of importance):\n",
    "   1. Solves the task or answers the question asked in the to-do. This means, if the code cells in the notebook are executed in order, we will get the output shown in your notebook.\n",
    "   1. The code is easy to follow and does not contain unnecessary steps.\n",
    "   1. The comments show that you understand how your solution works.\n",
    "   1. A very good answer will also provide code that is computationally efficient but easy to read.\n",
    "1. You can use any suitable publicly available libraries. Unless the task explicitly asks you to implement something from scratch, there is no penalty for using libraries to implement some steps.\n",
    "\n",
    "## Support\n",
    "\n",
    "The main source of support will be during the remaining lab sessions (Fridays 2-5pm) for this unit. \n",
    "\n",
    "The TAs and lecturer will help you with questions about the lectures, the code provided for you in this notebook, and general questions about the topics we cover. For the marked 'to-dos', they can only answer clarifying questions about what you have to do. \n",
    "\n",
    "Office hours: You can book office hours with Edwin on Tuesdays 3pm-5pm by sending him an email (edwin.simpson@bristol.ac.uk). If those times are not possible for you, please contact him by email to request an alternative. \n",
    "\n",
    "## Deadline\n",
    "\n",
    "The notebook must be submitted along with the second notebook on Blackboard before **Wednesday 11th May at 13.00**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f9000-3bef-4148-8e8b-02db90140d0c",
   "metadata": {},
   "source": [
    "# 0. Packages\n",
    "\n",
    "There are some additional packages you need for this lab. There are two options: set up a new environment using the crossplatform_environment.yml file provided on Blackboard, or install the extra packages into your existing data_analytics Conda environment using, e.g., `conda install pytorch`. The packages are:\n",
    "  * pytorch=1.9.0\n",
    "  * scipy=1.8.0\n",
    "  * numpy=1.22.2\n",
    "  * tqdm=4.62.3\n",
    "  * transformers=2.1.1\n",
    "  * matplotlib-base=3.5.1\n",
    "  * matplotlib-inline=0.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549d0b9-84d1-4d35-80b1-6341a3ea7ce6",
   "metadata": {},
   "source": [
    "# 1. Word Embeddings (max. 26 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1a8852-b72a-48dd-a266-25a4bb774f6a",
   "metadata": {},
   "source": [
    "In this section we will use both sparse vectors and dense word2vec embeddings to obtain\n",
    "vector representations of words and documents. \n",
    "\n",
    "First, we will load the `tweet eval` sentiment dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5519c4b9-1cac-4e8a-9e7e-a8a40b389bdf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e599bf09-ed1a-4e40-a551-2e3a889a3487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache\\tweet_eval\\sentiment\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with 45615 instances loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache\\tweet_eval\\sentiment\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development/validation dataset with 2000 instances loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache\\tweet_eval\\sentiment\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset with 12284 instances loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45615/45615 [00:05<00:00, 9071.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary has 43358 words\n",
      "Index of \"happy\" is 17184\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"sentiment\",\n",
    "    split=\"train\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")\n",
    "\n",
    "\n",
    "dev_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"sentiment\",\n",
    "    split=\"validation\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "print(f\"Development/validation dataset with {len(dev_dataset)} instances loaded\")\n",
    "\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"sentiment\",\n",
    "    split=\"test\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")\n",
    "\n",
    "# Put the data into lists ready for the next steps...\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    train_texts.append(train_dataset[i]['text'])\n",
    "    train_labels.append(train_dataset[i]['label'])\n",
    "            \n",
    "# HINT: A count vectorizer object may be useful in later steps\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "# Get the vocabulary\n",
    "vocab = vectorizer.vocabulary_\n",
    "vocab_size = len(vocab)\n",
    "print(f'The vocabulary has {vocab_size} words')\n",
    "\n",
    "# invert the vocabulary dictionary so we can look up word types given an index\n",
    "keys = vocab.values()\n",
    "values = vocab.keys()\n",
    "vocab_inverted = dict(zip(keys, values))\n",
    "\n",
    "print(f'Index of \"happy\" is {vocab[\"happy\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a63c3c-d24c-4810-9c34-4532b845e310",
   "metadata": {},
   "source": [
    "## 1.1. Term-Document Matrix\n",
    "\n",
    "First we are going to obtain sparse word vectors from a term-document matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd30abb-0951-46ee-81a6-b9217fba6b90",
   "metadata": {},
   "source": [
    "**TO-DO 1.1a**: Create a term-document matrix for the training set. **Rows must correspond to terms, and columns to documents.** **(2 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a52da7-89a5-41d6-bfab-83b3231d6949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43358, 45615)\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR ANSWER HERE\n",
    "termDocMat_train = vectorizer.transform(train_texts).T\n",
    "print(termDocMat_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904d8c39-99e0-4113-901f-b4cd72b2e369",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 1.1b:** Write a function that takes a word as an argument and returns its term vector from the term-document matrix you computed. Get the term vector for 'happy' **as a row vector**. **(3 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2495952-e973-4b68-b219-590791740daf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 45615)\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR ANSWER HERE\n",
    "def get_termVector(term):\n",
    "     return termDocMat_train[vocab[term]]\n",
    "happy_vec = get_termVector(\"happy\")\n",
    "print(happy_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb324d8-a92a-4fd7-bd7f-f3b0f0958e6b",
   "metadata": {},
   "source": [
    "**TODO 1.1c:** Use the term vectors given by the term-document matrix to identify the five most similar words to 'happy'. **(3 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a2cca67-685e-4760-92be-eea50ed87769",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1966', 31.25699921617557), ('aoo', 31.272991542223778), ('betch', 31.272991542223778), ('cutie', 31.272991542223778), ('happybirthdayprincegeorge', 31.272991542223778)]\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist  # this may be a useful function\n",
    "# I will use euclidean distance to comput the similarity\n",
    "# WRITE YOUR OWN CODE HERE\n",
    "def get_Top5Similar(term_docMaxtirx, term):\n",
    "    dis_idx_list = []\n",
    "    target_v = get_termVector(term)\n",
    "    for index, term_v in enumerate(term_docMaxtirx):\n",
    "        dis_idx_list.append((index, cdist(target_v.A, term_v.A, 'euclidean')[0][0]))\n",
    "    top5_similar = [(vocab_inverted[i[0]],i[1]) for i in sorted(dis_idx_list, key=lambda x:x[1])[1:6]]  # remove the detected term\n",
    "    return top5_similar\n",
    "\n",
    "top5_happy_term = get_Top5Similar(termDocMat_train, 'happy')\n",
    "print(top5_happy_term)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9654fde-cbe7-4020-99f7-49d97b4eacfa",
   "metadata": {},
   "source": [
    "## 1.2 Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e772ce7-853e-478d-9d0a-3f591100e88d",
   "metadata": {},
   "source": [
    "Now, we will use Gensim to train a word2vec model. The code below tokenizes the training texts, then runs word2vec (the skipgram model) to learn a set of embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b5d13e1-683f-41f6-bf24-7c036aef59fa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "tokenized_texts = [list(tokenize(text)) for text in train_texts]\n",
    "emb_model = word2vec.Word2Vec(tokenized_texts, sg=1, min_count=1, window=3, vector_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "989624ac-dc98-461f-a5ad-c28cf27ca085",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# get the word vector for 'happy'\n",
    "happy_embedding = emb_model.wv['happy']\n",
    "print(happy_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccd66ea-cbbe-4c1a-a467-5a784facf33b",
   "metadata": {},
   "source": [
    "TODO 1.2a: Find the five most similar words to 'happy' according to your word2vec model. You can use the Gensim function `similar_by_word` to do this. (this task is unmarked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc37a093-db74-4e10-9224-0740381b30fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cute', 0.8785979151725769), ('busy', 0.8619716167449951), ('sweet', 0.8607420325279236), ('idc', 0.8588870167732239), ('blessed', 0.8585744500160217)]\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR OWN CODE HERE\n",
    "print(emb_model.wv.similar_by_word('happy',topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb28de29-ee93-47e9-ba62-de7771d7203d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Above, we trained our own model using the skipgram method. We can also download a pretrained model that has previously been trained on a large corpus. There is a list of models available [here](https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models). Let's try out GLoVe embeddings. GLoVe is an alternative to the skipgram model. This model was trained on a corpus of tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "693126a6-519a-44c1-b800-773fc95d38df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.2304   0.48312  0.14102 -0.0295  -0.65253 -0.18554  2.1033   1.7516\n",
      " -1.3001  -0.32113 -0.84774  0.41995 -3.8823   0.19638 -0.72865 -0.85273\n",
      "  0.23174 -1.0763  -0.83023  0.10815 -0.51015  0.27691 -1.1895   0.98094\n",
      " -0.13955]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "glove_wv = gensim.downloader.load('glove-twitter-25')\n",
    "\n",
    "# show the vector for Hamlet:\n",
    "print(glove_wv['happy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcbeb21-549d-4315-bce1-e785cda13fd2",
   "metadata": {},
   "source": [
    "TODO 1.2b: Find the most similar five words to 'happy' according to the GloVe Twitter model. (this task is unmarked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6893470a-08b7-4f44-b5f0-8faddb91ae85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('birthday', 0.9577818512916565), ('thank', 0.937666654586792), ('welcome', 0.93361496925354), ('love', 0.917618453502655), ('miss', 0.9164499640464783)]\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR OWN CODE HERE\n",
    "print(glove_wv.similar_by_word('happy',topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd4dc0e-7c98-49df-a316-3d67b5a95ca1",
   "metadata": {},
   "source": [
    "# 1.3 Tweet Embeddings\n",
    "\n",
    "For many tasks it is useful to obtain a document embedding that characterises the meaning of a document. With short documents, such as tweets, a reasonable way to obtain a document embedding is to average the word embeddings of the tokens in the tweet.\n",
    "\n",
    "**TO-DO 1.3a:** Compute average word embeddings for each tweet in the `tokenized_texts` list. You can use any of the word embeddings we have generated so far. **(4 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ddecdad-b333-4a71-abf7-16a37fda14c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45615, 25)\n",
      "[ 0.2071541   0.2363972   0.24716364 -0.07159602  0.10986491 -0.3843282\n",
      "  1.2351846  -0.08526091  0.14360726 -0.4218817   0.26150545  0.4071491\n",
      " -4.5810003   0.31376487  0.20031352 -0.30543864  0.37240162  0.13152811\n",
      " -0.07848     0.01895846 -0.7695406  -0.06387872 -0.12701182 -0.31038758\n",
      " -0.32766664]\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR OWN CODE HERE\n",
    "def getDocVec (token_texts, model):\n",
    "    doc_embedding_list = []\n",
    "    feature_size = model.vector_size\n",
    "    for doc in token_texts:\n",
    "        doc_embedding = []\n",
    "        for token in doc:\n",
    "            if token in model:\n",
    "                try:\n",
    "                    doc_embedding.append(model[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if doc_embedding:\n",
    "            doc_embedding = np.array(doc_embedding)\n",
    "            doc_embedding_list.append(doc_embedding.mean(axis=0))\n",
    "        else:\n",
    "            doc_embedding_list.append(np.zeros(feature_size))\n",
    "    return doc_embedding_list\n",
    "\n",
    "    \n",
    "tweet_embedding_list = getDocVec(tokenized_texts,glove_wv)\n",
    "print(np.array(tweet_embedding_list).shape)\n",
    "print(tweet_embedding_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f7b648-b65c-4c2f-bc34-c3ce1cf362cb",
   "metadata": {},
   "source": [
    "**TO-DO 1.3b:** What are the limitations of using average word embeddings to represent documents? Explain in a couple of sentences. You can write your answer inside this cell. **(4 marks)**\n",
    "\n",
    "WRITE YOUR ANSWER HERE  \n",
    "  \n",
    "Simply averaging all word vectors of a sentence as sentence representation can not reflect the importance of words in the sentence, and it also ignores the order of words in the sentence. For example: \"Manchester City beat Real Madrid\" and \"Real Madrid beat Manchester City\" These two different meaning sentences will have the same document vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8938c15-1aa4-4a2f-b1e6-33c5155fb085",
   "metadata": {},
   "source": [
    "**TO-DO 1.3c:** Use the embeddings to organise the tweets in `tokenized_texts` into topic categories. There are no gold labels for the categories, so you will need to choose a method that does not require them. Show or visualise the words that are most strongly associated with each category. Based on these words, can you identify a topic name for any of the categories? Describe what you find in a couple of lines below. **(10 marks)**\n",
    "\n",
    "WRITE YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3e6f085d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: ['saturday', 'right', 'sunday', 'thursday', 'monday', 'good', 'love', 'time', 'gonna', 'night', 'got', 'said', 'going', 'think', 'day', 'friday', 'user', 'know', 'tomorrow', 'like']\n",
      "Topic 2: ['3rd', 'game', '2nd', 'need', 'monday', 'll', 'amp', 'night', 'good', '1st', 'saturday', 'want', 'friday', 'day', 'new', 'sunday', 'going', 'time', 'tomorrow', 'user']\n",
      "Topic 3: ['thursday', 'ready', 'monday', 'tonight', 'wait', 'need', 'tickets', 'game', 'come', 'sunday', 'watch', 'friday', 'night', 'saturday', 'time', 'user', 'day', 'want', 'going', 'tomorrow']\n",
      "Topic 4: ['saturday', 'monday', 'game', 'going', 'friday', 'sunday', 'new', 'vs', 'gt', 'tonight', '2nd', '3rd', 'night', 'day', '1st', 'tomorrow', 'u2019s', 'amp', 'u002c', 'user']\n",
      "Topic 5: ['lol', 'sunday', 'll', 'wait', 'birthday', 'come', 'saturday', 'happy', 'u2019t', 'like', 'friday', 'day', 'u2019s', 'know', 'love', 'want', 'going', 'u002c', 'tomorrow', 'user']\n",
      "Topic 6: ['sun', 'love', 'live', 'tonight', '3rd', 'thursday', '1st', 'sunday', 'saturday', 'night', '2nd', 'new', 'amp', 'liked', 'day', 'friday', 'rt', 'video', 'tomorrow', 'user']\n",
      "Topic 7: ['album', '3rd', 'live', 'tonight', '2nd', 'vs', 'time', 'saturday', 'friday', 'sunday', '1st', 'new', 'night', 'day', 'game', 'going', 'tomorrow', 'u2019s', 'user', 'u002c']\n",
      "Topic 8: ['kanye', 'god', 'better', 'want', 'day', 'said', '1st', 'good', 'islam', 'amp', 'christians', 'time', 'thing', 'tomorrow', 'muslims', 'know', 'people', 'think', 'like', 'user']\n",
      "Topic 9: ['monday', 'time', 'sat', 'think', 'll', 'sunday', 'got', 'lol', 'amp', 'u2019s', 've', 'u2019t', 'know', '1st', 'friday', 'tomorrow', '2nd', 'u002c', 'like', 'user']\n",
      "Topic 10: ['arabia', 'thursday', 'israel', 'wednesday', 'sun', 'tuesday', 'thine', 'friday', 'haram', 'boko', 'saudi', 'bags', '1st', 'monday', 'new', 'iran', 'sunday', 'gucci', 'user', 'amp']\n",
      "Topic 11: ['today', 'got', 'morning', 'hot', '1st', 'ice', '2nd', 'monday', 'watching', 'saturday', 'sunday', 'sat', 'friday', 'like', 'day', 'night', 'sun', 'tomorrow', 'amp', 'user']\n",
      "Topic 12: ['2012', 'today', 'thursday', '2015', '4th', 'july', 'world', 'sun', 'september', 'monday', 'season', 'best', 'game', 'saturday', 'sunday', 'night', '1st', 'friday', 'day', 'new']\n",
      "Topic 13: ['3rd', 'great', 'watching', 'user', 'new', 'sunday', 'weekend', 'come', 'concert', 'friday', 'tickets', 'saturday', 'today', 'going', 'game', 'time', 'tonight', 'day', 'night', 'tomorrow']\n",
      "Topic 14: ['league', 'kris', 'sunday', 'round', 'murray', 'bryant', 'white', 'lead', 'inning', '4th', 'run', '2nd', 'season', 'win', 'david', '3rd', 'vs', 'sox', '1st', 'game']\n",
      "Topic 15: ['watching', 'sun', '4th', 'saturday', '3rd', 'new', 'monday', 'today', 'game', 'like', '2nd', 'tomorrow', 'night', 'friday', 'sunday', 'time', 'best', '1st', 'user', 'day']\n",
      "Topic 16: ['obama', 'scotus', 'sunday', 'think', 'like', 'ira', 'friday', 'iran', 'marriage', 'said', 'muslims', '3rd', 'gay', 'islam', 'says', 'yakub', '2nd', '1st', 'amp', 'user']\n",
      "Topic 17: ['appeared', 'night', 'top20', 'thursday', 'trends', 'apple', 'saturday', 'tomorrow', 'day', 'news', 'sunday', 'place', 'friday', '3rd', 'album', 'event', 'amp', '1st', 'new', 'user']\n",
      "Topic 18: ['friday', 'saturday', 'school', 'work', 'wait', 'today', 'excited', 'im', 'morning', 'good', 'watch', 'll', 'time', 'gonna', 'tonight', 'day', 'user', 'night', 'going', 'tomorrow']\n",
      "Topic 19: ['christians', 'time', 'thursday', 'john', 'iran', 'sun', 'friday', 'says', 'arabia', 'world', 'president', 'monday', 'saudi', '1st', 'said', 'muslims', 'haram', 'boko', 'user', 'sunday']\n",
      "Topic 20: ['want', '2nd', 'wait', 'happy', 'birthday', 'saturday', 'love', 'sunday', 'night', 'u2019ll', 'friday', 'u2019', 'going', 'day', 'tomorrow', 'u2019m', 'u2019t', 'user', 'u2019s', 'u002c']\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR OWN CODE HERE\n",
    "# Use Kmeans to cluster sentences, set 20 topics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "topic_num = 20\n",
    "kmeans = KMeans(n_clusters=topic_num)\n",
    "kmeans.fit(tweet_embedding_list)\n",
    "topic_list = kmeans.labels_\n",
    "bows_topic = []\n",
    "# Use tf_idf to reflect the importance of each words in sentence, \n",
    "# I think if If we simply sort by word frequency, the most relevant words will be \n",
    "# the words that are not so important but appear frequently in many sentences, \n",
    "# such as \"like\" ,\"come\" and some stopwords.  \n",
    "tf_idf = TfidfVectorizer()\n",
    "tfidf_documents = tf_idf.fit_transform(train_texts)\n",
    "vocab_tf_idf = tf_idf.vocabulary_\n",
    "keys = vocab_tf_idf.values()\n",
    "values = vocab_tf_idf.keys()\n",
    "vocab_tfidf_inverted = dict(zip(keys, values))\n",
    "for i in range(topic_num):\n",
    "    sentences_in_topic = []\n",
    "    for idx, topic in enumerate(topic_list):\n",
    "        if topic == i:\n",
    "            sentences_in_topic.append(tfidf_documents[idx])\n",
    "    bows_topic.append(sentences_in_topic)\n",
    "top10words_Topics = []\n",
    "for i, bows in enumerate(bows_topic):\n",
    "    sorted_idx = np.sum(bows).A.argsort()[0]\n",
    "    # remove stopwords when searching words\n",
    "    top10_words = [vocab_tfidf_inverted[idx] for idx in sorted_idx if vocab_tfidf_inverted[idx] not in STOPWORDS][-20:]\n",
    "    top10words_Topics.append(top10_words)\n",
    "    print(f\"Topic {i+1}: {top10_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbb2894",
   "metadata": {},
   "source": [
    "In these 20 topics,  \n",
    "Topic 8: ['kanye', 'god', 'better', 'want', 'day', 'said', '1st', 'good', 'islam', 'amp', 'christians', 'time', 'thing', 'tomorrow', 'muslims', 'know', 'people', 'think', 'like', 'user']  may be relevant to Religion\n",
    "\n",
    "Topic 19: ['christians', 'time', 'thursday', 'john', 'iran', 'sun', 'friday', 'says', 'arabia', 'world', 'president', 'monday', 'saudi', '1st', 'said', 'muslims', 'haram', 'boko', 'user', 'sunday'] may be relevant to Politics or Wars \n",
    "\n",
    "But, I have to admit the most assciated words of topics are filled with time related words, which causes I can not recognize more topic categories from these clusters. This proves the search for the most relevant words needs more complex processing than what I have done and averaging word embeddings in sentences is not the best method to represent sentence meaning.  \n",
    "\n",
    "There may be two potential ways to improve the performance of topic modeling (clustering) using embedding:  \n",
    "1. Use more reasonable methods to do sentence embedding like applying tf-idf idea to it or directly advanced model like [Siamese-BERT](https://arxiv.org/pdf/1908.10084.pdf) ([code link](https://github.com/UKPLab/sentence-transformers))\n",
    "1. Meanwhile, Chris Moody introduced a method to combine embeddings and LDA model to do topic modeling [lda2vec](https://github.com/cemoody/lda2vec). Maybe this is also a way worth exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421d24e4-d90d-4f50-a167-5f4876076355",
   "metadata": {},
   "source": [
    "# 2. Introducing Neural Text Classifiers (max. 18 marks)\n",
    "\n",
    "This section shows you how to implement a neural network classifier using Pytorch and leads you through the steps required to process text sequences.\n",
    "\n",
    "There are several big advantages to building a text classifier using a neural network:\n",
    "   * It can model nonlinear functions, so can handle much more complex relationships between features and class labels.\n",
    "   * It performs representation learning: the hidden layers learn how to extract features from low-level data.\n",
    "   * It can process sequences of tokens -- we don't have to think in terms of a single feature vector representing a document as we did for logistic regression.\n",
    "  \n",
    "The downsides are:\n",
    "   * Much more expensive to train and test.\n",
    "   * It can overfit very badly to small datasets.\n",
    "   * The features learned by the hidden layers can be hard to interpret.\n",
    "   \n",
    "Let's start by building a neural network text classifier that takes a sequence of tokens as input, and predicts a class label. For simplicity, it will use a single fully connected feedforward layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca4f88e-bd86-4917-8e9d-26ace635873e",
   "metadata": {},
   "source": [
    "The first step -- as always -- is to get our data into the right format. We start from a set of tokenised documents (in this case, tweets), where each document is represented as a sequence of text tokens. The neural network cannot process the tokens as strings, so we need to convert them to numerical data.\n",
    "\n",
    "We are going to construct the neural network in this form:\n",
    "\n",
    "<img src=\"neural_text_classifier_smaller.png\" alt=\"Neural text classifier diagram\" width=\"600px\"/>\n",
    "\n",
    "The input value for each token is used to look up the corresponding embedding in the embedding layer. For PyTorch, it's not necessary to create the one-hot vectors as the embedding lookup is handled inside the library. Instead, we just need to give the indexes of the words in the vocabulary.\n",
    "\n",
    "So, let's now map the tokens to their IDs -- their indexes in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "42a71101-e504-42a8-9862-74796af52cd0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97adf45d8aeb422e84900588f51f1a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45615 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "45615"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize training set and convert to input IDs.\n",
    "def encode_text(sample):\n",
    "    tokens = tokenize(sample['text'])  # Tokenize one document\n",
    "    \n",
    "    input_ids = []\n",
    "    for token in tokens:\n",
    "        if str.lower(token) in vocab:  # Skip words from the dev/test set that are not in the vocabulary.\n",
    "            input_ids.append(vocab[str.lower(token)]+1) # +1 is needed because we reserve 0 as a special character\n",
    "            \n",
    "    sample['input_ids'] = input_ids \n",
    "    return sample\n",
    "\n",
    "# The map method of the dataset object takes a function as its argument, \n",
    "# and applies that function to each document in the dataset.\n",
    "train_dataset = train_dataset.map(encode_text)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce8d8c-ce83-49b7-9b88-b84233be7dbf",
   "metadata": {},
   "source": [
    "Our neural network's input layer has a fixed size, so we need to make all of our documents have the same number of tokens. Let's plot a histogram to understand the length distribution of the texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "832da949-3fec-4d26-936e-23c943546b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the document length: 18.160166611860134\n",
      "Median of the document length: 18.0\n",
      "Maximum document length: 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([   21.,   522.,  2430.,  4908.,  7772., 11003., 10389.,  6738.,\n",
       "         1719.,   113.]),\n",
       " array([ 1. ,  4.1,  7.2, 10.3, 13.4, 16.5, 19.6, 22.7, 25.8, 28.9, 32. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rv_l = [len(doc) for doc in train_dataset['input_ids']]\n",
    "print('Mean of the document length: {}'.format(np.mean(rv_l)))\n",
    "print('Median of the document length: {}'.format(np.median(rv_l)))\n",
    "print('Maximum document length: {}'.format(np.max(rv_l)))\n",
    "plt.hist(rv_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.,     0.,  4832., 34982., 34982.,  8964., 31612., 27815.,\n",
       "       27218., 37895., 22531., 38278., 10024., 26417., 34676.])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros_app = np.zeros(2)\n",
    "np.append(zeros_app,train_dataset[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da97d92c-7002-4b63-9011-8ed21ea52e95",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now neeed to choose a fixed sequence length, then *pad* the documents that are shorter than this maximum by adding a special token to the start of the sequence. Any documents that exceed the length will be truncated.\n",
    "\n",
    "**TO-DO 2a:** Complete the padding code below to insert 0s at the start of any sequences that are too short, and to truncate any sequences that are too long. **(3 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a8488dfa-61b8-438e-92ee-7f9b84b27a68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87880dc812064cc4afea13876255e4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45615 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# truncate all docs longer than this. Pad all docs shorter than this.\n",
    "sequence_length = 40\n",
    "def pad_text(sample):\n",
    "    # WRITE YOUR OWN CODE HERE\n",
    "    if len(sample['input_ids']) > sequence_length:\n",
    "        sample['input_ids'] = sample['input_ids'][:sequence_length]\n",
    "    else:\n",
    "        dif = sequence_length - len(sample['input_ids'])\n",
    "        zeros_app = np.zeros(dif)\n",
    "        sample['input_ids'] = np.append(zeros_app,sample['input_ids'])\n",
    "    return sample\n",
    "\n",
    "\n",
    "# The map method will call pad_text for every document in the dataset\n",
    "train_dataset = train_dataset.map(pad_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb33a1-ca74-4a95-bbd5-2b9e50167733",
   "metadata": {},
   "source": [
    "We now have our data in almost the right format! To train a model using PyTorch, we are going to wrap our dataset in a [DataLoader object](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). This allows the training process to select random subsets of the dataset -- mini-batches -- which it will use for learning with a mini-batch stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4ab3c226-402b-4eb5-8f9a-fe216b4c5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# convert from the Huggingface format to a TensorDataset so we can use the mini-batch sampling functionality\n",
    "def convert_to_data_loader(dataset, num_classes):\n",
    "    # convert from list to tensor\n",
    "    input_tensor = torch.from_numpy(np.array(dataset['input_ids']))\n",
    "    label_tensor = torch.from_numpy(np.array(dataset['label'])).long()\n",
    "    tensor_dataset = TensorDataset(input_tensor, label_tensor)\n",
    "    loader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return loader\n",
    "\n",
    "num_classes = len(np.unique(train_labels))   # number of possible labels in the sentiment analysis task\n",
    "\n",
    "train_loader = convert_to_data_loader(train_dataset, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a5825c-4a0e-4fc8-9de6-504bc7227377",
   "metadata": {},
   "source": [
    "Let's process the development and test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "78aa6ddc-b936-431c-865e-dc55eaf3086c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c35bf9430240cdbcf540c90796317d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43b0ad86ae549cf92ca194ae5d19eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c34fe807e24ad5a5cf53f9507ece10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12284 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7b56a3b70745cea255db7adace3e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12284 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_dataset = dev_dataset.map(encode_text)\n",
    "dev_dataset = dev_dataset.map(pad_text)\n",
    "dev_loader = convert_to_data_loader(dev_dataset, num_classes)\n",
    "\n",
    "test_dataset = test_dataset.map(encode_text)\n",
    "test_dataset = test_dataset.map(pad_text)\n",
    "test_loader = convert_to_data_loader(test_dataset, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7b857f-5d57-44fd-a1b4-e1b31a009c72",
   "metadata": {},
   "source": [
    "As shown in the diagram above, we will build a NN with three different layers for sentiment classification.\n",
    "\n",
    "### Embedding layer\n",
    "In the embedding layer, the network will create its own embeddings for the index with a given embedding dimension.\n",
    "The module `nn.Embedding()` creates a simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "This module is often used to store word embeddings and retrieve them using indices.\n",
    "The module's input is a list of indices, and the output is the corresponding word embeddings.\n",
    "\n",
    "[Documentation for Embedding Class](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
    "\n",
    "### Fully-connected layer\n",
    "Fully-connected layers in a neural network are those layers where all the inputs from the previous layer are connected to every unit of the fully-connected layer. Here we will use fully-connected layers for the hidden layer and output layer. In Pytorch this kind of layer is implemented by the 'Linear' class:\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "\n",
    "## Activation functions\n",
    "In Pytorch, the activation function is not included in the Linear class (or other kinds of neural network layer).\n",
    "In Pytorch, we construct a neural network by connecting up the output of each component to the input of the next, thereby creating a computation graph.\n",
    "To complete the hidden layer, we connect the ouput of the linear layer to a ReLU activation function, thereby creating a nonlinear function.\n",
    "\n",
    "The cell below defines a class for our neural text classifier. The constructor creates each of the layers and the activations. The dimensions of each layer need to be correct so that the output of one layer can be passed as input to the next.\n",
    "\n",
    "**TO-DO 2b** Complete the constructor below for a NN with three layers by adding the missing dimensions. **(2 marks)**\n",
    "\n",
    "Below is the forward method. This is called to map the neural network's inputs to its outputs. In PyTorch, we pass data through each layer of the model, connecting them together.\n",
    "\n",
    "**TO-DO 2c** Complete the forward method by adding the missing line. **(2 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e38a13cf-4b4d-4ac2-868c-8ad5c1b72b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FFTextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, sequence_length, embedding_size, hidden_size, num_classes):\n",
    "        super(FFTextClassifier, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "        #self.sequence_length = sequence_length\n",
    "        # Here we just need to construct the components of our network. We don't need to connect them together yet.\n",
    "        ### COMPLETE THE CODE HERE: WRITE IN THE MISSING ARGUMENTS SPECIFYING THE DIMENSIONS OF EACH LAYER\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_size) # embedding layer\n",
    "        self.hidden_layer = nn.Linear(embedding_size * sequence_length, hidden_size) # Hidden layer\n",
    "        self.activation = nn.ReLU(True) # Hidden layer\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes) # Fully connected layer\n",
    "        \n",
    "        ##########\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "        # Input dimensions are:  (batch_size, seq_length)\n",
    "        embedded_words = self.embedding_layer(input_words)  # (batch_size, seq_length, embedding_size)\n",
    "        # flatten the sequence of embedding vectors for each document into a single vector.\n",
    "        embedded_words = embedded_words.reshape(embedded_words.shape[0], sequence_length*self.embedding_size)  # batch_size, seq_length*embedding_size\n",
    "        ### ADD THE MISSING LINES HERE TO COMPUTE h\n",
    "        h = self.hidden_layer(embedded_words)\n",
    "        ########\n",
    "        output = self.output_layer(h)                      # (batch_size, num_classes)\n",
    "        # Notice we haven't applied a softmax activation to the output layer -- it's not required by Pytorch's loss function.\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06f22db-b953-4f98-920f-3f9a2b1e3707",
   "metadata": {},
   "source": [
    "Now the class is complete...\n",
    "\n",
    "**TO-DO 2d** Create a NN with the FFTextClassifier class we wrote. **(1 mark)**\n",
    "\n",
    "**Hint:** `model = FFTextClassifier(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a52d795a-adbc-4c48-aadf-f911520ac42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vectorizer.vocabulary_) + 1\n",
    "embedding_size = 10  # number of dimensions for embeddings\n",
    "hidden_size = 8  # number of hidden units\n",
    "# WRITE YOUR OWN CODE HERE\n",
    "model = FFTextClassifier(vocab_size, sequence_length, embedding_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a6c48d-f2b4-45e6-838e-d08c895f7e86",
   "metadata": {},
   "source": [
    "After desigining our network, we need to create a training function to calculate the loss for each input and perform backpropagation to optimise the network.\n",
    "During training, the weights of all the layers will be updated.\n",
    "\n",
    "Below, we build a training function to train the NN over a fixed number of epochs (an epoch is one iteration over the whole training dataset).\n",
    "The function also prints the performance of both training and development/validation set after each epoch.\n",
    "\n",
    "Here we use cross-entropy loss, which is the standard loss function for classification that we also used for logistic regression. The module `nn.CrossEntropyLoss()` operates directly on the output of our output layer, so we don't have to implement the softmax layer within the forward() method.\n",
    "\n",
    "Cross Entropy Loss: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "\n",
    "The optimizer object implements a particular algorithm for updating the weights. Here, we will use the Adam optimizer, which is a variant of stochastic gradient descent method that tends to find a better solution in a small number of iterations than standard SGD.\n",
    "\n",
    "Optimization: https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "**TO-DO 2e** Complete the code below to compute the validation accuracy and loss after each training epoch. **(2 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "62af0a20-79a5-49ca-882a-638a68b288f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "def train_nn(num_epochs, model, train_dataloader, dev_dataloader):\n",
    "    \n",
    "    learning_rate = 0.0005  # learning rate for the gradient descent optimizer, related to the step size\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()  # create loss function object\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # create the optimizer\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        # Track performance on the training set as we are learning...\n",
    "        total_correct = 0\n",
    "        total_trained = 0\n",
    "        train_losses = []\n",
    "\n",
    "        model.train()  # Put the model in training mode.\n",
    "\n",
    "        for i, (batch_input_ids, batch_labels) in enumerate(train_dataloader):\n",
    "            # Iterate over each batch of data\n",
    "            # print(f'batch no. = {i}')\n",
    "\n",
    "            optimizer.zero_grad()  # Reset the optimizer\n",
    "\n",
    "            # Use the model to perform forward inference on the input data.\n",
    "            # This will run the forward() function.\n",
    "            output = model(batch_input_ids)\n",
    "\n",
    "            # Compute the loss for the current batch of data\n",
    "            batch_loss = loss_fn(output, batch_labels)\n",
    "\n",
    "            # Perform back propagation to compute the gradients with respect to each weight\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # Update the weights using the compute gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record the loss from this sample to keep track of progress.\n",
    "            train_losses.append(batch_loss.item())\n",
    "\n",
    "            # Count correct labels so we can compute accuracy on the training set\n",
    "            predicted_labels = output.argmax(1)\n",
    "            total_correct += (predicted_labels == batch_labels).sum().item()\n",
    "            total_trained += batch_labels.size(0)\n",
    "\n",
    "        train_accuracy = total_correct/total_trained*100\n",
    "\n",
    "        print(\"Epoch: {}/{}\".format((e+1), num_epochs),\n",
    "              \"Training Loss: {:.4f}\".format(np.mean(train_losses)),\n",
    "              \"Training Accuracy: {:.4f}%\".format(train_accuracy))\n",
    "\n",
    "        model.eval()  # Switch model to evaluation mode\n",
    "        total_correct = 0\n",
    "        total_trained = 0\n",
    "        dev_losses = []\n",
    "\n",
    "        for dev_input_ids, dev_labels in dev_dataloader:\n",
    "            ### WRITE YOUR OWN CODE HERE ##############\n",
    "            dev_output = model(dev_input_ids)\n",
    "            dev_loss = loss_fn(dev_output, dev_labels)\n",
    "            ###########################################\n",
    "\n",
    "            # Save the loss on the dev set\n",
    "            dev_losses.append(dev_loss.item())\n",
    "\n",
    "            # Count the number of correct predictions\n",
    "            predicted_labels = dev_output.argmax(1)\n",
    "            total_correct += (predicted_labels == dev_labels).sum().item()\n",
    "            total_trained += dev_labels.size(0)\n",
    "            \n",
    "        dev_accuracy = total_correct/total_trained*100\n",
    "        \n",
    "        print(\"Epoch: {}/{}\".format((e+1), num_epochs),\n",
    "              \"Validation Loss: {:.4f}\".format(np.mean(dev_losses)),\n",
    "              \"Validation Accuracy: {:.4f}%\".format(dev_accuracy))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e473f92-78d4-4a5d-94cc-749165e3896e",
   "metadata": {},
   "source": [
    "**TO-DO 2f:** Why is it useful to keep track of the training and dev/validation set loss or performance at each epoch? How could we use this information during training? **(3 marks)**\n",
    "\n",
    "EXPLAIN YOUR ANSWER HERE\n",
    "\n",
    "Monitoring indicators such as loss rate and accuracy can be used to identify the training state of the model. According to the records of these indicators, we can tune the hyperparameters, like learning rate, batch size, number of iterations (epochs), the type of activation function and so on, so as to improve the utilisation of resources and get the model with a better generalisation capability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b639b1e0-c968-4086-ada2-21df7100acb9",
   "metadata": {},
   "source": [
    "TO-DO 2g: Finally, train the network for 10 epochs! (this to-do will not be marked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "5ac7c289-fae8-4045-b18c-8db7e678214e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 Training Loss: 1.0159 Training Accuracy: 44.8427%\n",
      "Epoch: 1/10 Validation Loss: 1.0204 Validation Accuracy: 43.8000%\n",
      "Epoch: 2/10 Training Loss: 0.9987 Training Accuracy: 47.4362%\n",
      "Epoch: 2/10 Validation Loss: 1.0159 Validation Accuracy: 45.1000%\n",
      "Epoch: 3/10 Training Loss: 0.9773 Training Accuracy: 49.9507%\n",
      "Epoch: 3/10 Validation Loss: 0.9942 Validation Accuracy: 48.6500%\n",
      "Epoch: 4/10 Training Loss: 0.9479 Training Accuracy: 52.7765%\n",
      "Epoch: 4/10 Validation Loss: 0.9798 Validation Accuracy: 51.0000%\n",
      "Epoch: 5/10 Training Loss: 0.9161 Training Accuracy: 55.1178%\n",
      "Epoch: 5/10 Validation Loss: 0.9630 Validation Accuracy: 54.0500%\n",
      "Epoch: 6/10 Training Loss: 0.8848 Training Accuracy: 57.5512%\n",
      "Epoch: 6/10 Validation Loss: 0.9420 Validation Accuracy: 54.2500%\n",
      "Epoch: 7/10 Training Loss: 0.8559 Training Accuracy: 59.5111%\n",
      "Epoch: 7/10 Validation Loss: 0.9270 Validation Accuracy: 55.3000%\n",
      "Epoch: 8/10 Training Loss: 0.8297 Training Accuracy: 61.2518%\n",
      "Epoch: 8/10 Validation Loss: 0.9115 Validation Accuracy: 56.6500%\n",
      "Epoch: 9/10 Training Loss: 0.8042 Training Accuracy: 62.9069%\n",
      "Epoch: 9/10 Validation Loss: 0.9065 Validation Accuracy: 57.2500%\n",
      "Epoch: 10/10 Training Loss: 0.7825 Training Accuracy: 64.2135%\n",
      "Epoch: 10/10 Validation Loss: 0.8840 Validation Accuracy: 57.4500%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FFTextClassifier(\n",
       "  (embedding_layer): Embedding(43359, 10)\n",
       "  (hidden_layer): Linear(in_features=400, out_features=8, bias=True)\n",
       "  (activation): ReLU(inplace=True)\n",
       "  (output_layer): Linear(in_features=8, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### WRITE YOUR OWN CODE HERE\n",
    "num_epochs = 10\n",
    "train_nn(num_epochs, model, train_loader, dev_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d655f07b-c687-4d9b-a0b3-44f0d3ccbead",
   "metadata": {},
   "source": [
    "The code below obtains predictions from our neural network.\n",
    "\n",
    "**TO-DO 2h:** Evaluate the model on test set using the function below. Complete the code to compute a suitable performance metric for sentiment classification. **(2 mark)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "cf4368a8-57fc-4cd3-b74e-aa385105c91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.20      0.30      3972\n",
      "           1       0.54      0.74      0.62      5937\n",
      "           2       0.43      0.51      0.46      2375\n",
      "\n",
      "    accuracy                           0.52     12284\n",
      "   macro avg       0.53      0.48      0.46     12284\n",
      "weighted avg       0.55      0.52      0.49     12284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def predict_nn(trained_model, test_loader):\n",
    "\n",
    "    trained_model.eval()\n",
    "\n",
    "    correct = 0  # count the number of correct classification labels\n",
    "\n",
    "    gold_labs = []  # gold labels to return\n",
    "    pred_labs = []  # predicted labels to return\n",
    "    \n",
    "    for inputs, labels in test_loader:\n",
    "        test_output = trained_model(inputs)\n",
    "        predicted_labels = test_output.argmax(1)\n",
    "\n",
    "        gold_labs.extend(labels.tolist())\n",
    "        pred_labs.extend(predicted_labels.tolist())\n",
    "    \n",
    "    return gold_labs, pred_labs\n",
    "\n",
    "gold_labs, pred_labs = predict_nn(model, test_loader)\n",
    "\n",
    "### WRITE YOUR OWN CODE HERE\n",
    "print(classification_report(gold_labs, pred_labs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5620658-9e5a-4248-aa48-3e3868011aaf",
   "metadata": {},
   "source": [
    "Now, we can use pretrained word embeddings instead of learning them from scratch during training.\n",
    "Here, we will use the pretrained GloVe embeddings that we loaded before. The embedding matrix is used to initialise the embedding layer. The code below converts the GloVe embeddings into an embedding matrix suitable for PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "872638e9-f86c-48e4-8745-9c667e250a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.3535,  0.0987,  0.1718,  ...,  0.4630,  1.3101,  1.1314],\n",
      "        [-0.4106,  0.1487,  0.0637,  ...,  0.6097,  1.0935,  0.9614],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = torch.zeros((vocab_size, glove_wv.vector_size))\n",
    "for word in vocab:\n",
    "    word_idx = vocab[word]\n",
    "    if word in glove_wv:\n",
    "        embedding_matrix[word_idx, :] = torch.from_numpy(glove_wv[word])\n",
    "        \n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850e7656-343f-449c-9329-411ad7fb22d5",
   "metadata": {},
   "source": [
    "The class below extends the FFTextClassifier class. This means that it inherits all of its functionality, but we overwrite the constructor (the `__init__` method). This way, we don't need to define the forward function again, as it will be the same as before.\n",
    "\n",
    "The embedding layer is now different as it loads pretrained embeddings from our matrix. The argument `freeze` determines whether the embeddings remain fixed to their pretrained values (if `freeze=True`) or are updated through backpropagation to fit them to the dataset.\n",
    "\n",
    "**TO-DO 2i:** Complete the arguments below to set the dimensions of the neural network layers.  Repeat the experiment above using the FFTextClassifierWithEmbeddings with the GLoVe embeddings. Compare the performance of the two neural text classifiers, and explain in a couple of sentences why you think they perform differently. **(3 marks)**\n",
    "\n",
    "WRITE YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "09b93f97-57b9-4d3b-a36f-547d472e1f37",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "\n",
    "class FFTextClassifierWithEmbeddings(FFTextClassifier):\n",
    "\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(FFTextClassifier, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_matrix.shape[1] \n",
    "\n",
    "        # Here we just need to construct the components of our network. We don't need to connect them together yet.\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_matrix, freeze=True) # embedding layer\n",
    "\n",
    "        ### COMPLETE THE ARGUMENTS TO SPECIFY THE DIMENSIONS OF THE LAYERS\n",
    "        self.hidden_layer = nn.Linear(self.embedding_size*sequence_length, hidden_size) # Hidden layer\n",
    "        self.activation = nn.ReLU(True) # Hidden layer\n",
    "        self.output_layer = nn.Linear(hidden_size,num_classes) # Full connection layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "73d4afd4-a92d-468d-a6ea-e693c10de36d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 Training Loss: 1.0020 Training Accuracy: 46.7631%\n",
      "Epoch: 1/10 Validation Loss: 0.9926 Validation Accuracy: 46.4500%\n",
      "Epoch: 2/10 Training Loss: 0.9747 Training Accuracy: 49.0957%\n",
      "Epoch: 2/10 Validation Loss: 0.9900 Validation Accuracy: 47.3500%\n",
      "Epoch: 3/10 Training Loss: 0.9682 Training Accuracy: 49.8148%\n",
      "Epoch: 3/10 Validation Loss: 0.9934 Validation Accuracy: 47.1500%\n",
      "Epoch: 4/10 Training Loss: 0.9657 Training Accuracy: 50.0252%\n",
      "Epoch: 4/10 Validation Loss: 0.9910 Validation Accuracy: 47.2000%\n",
      "Epoch: 5/10 Training Loss: 0.9639 Training Accuracy: 50.1633%\n",
      "Epoch: 5/10 Validation Loss: 0.9900 Validation Accuracy: 47.4000%\n",
      "Epoch: 6/10 Training Loss: 0.9625 Training Accuracy: 50.1831%\n",
      "Epoch: 6/10 Validation Loss: 0.9869 Validation Accuracy: 47.9000%\n",
      "Epoch: 7/10 Training Loss: 0.9616 Training Accuracy: 50.1809%\n",
      "Epoch: 7/10 Validation Loss: 0.9915 Validation Accuracy: 47.8500%\n",
      "Epoch: 8/10 Training Loss: 0.9609 Training Accuracy: 50.4593%\n",
      "Epoch: 8/10 Validation Loss: 0.9866 Validation Accuracy: 47.0000%\n",
      "Epoch: 9/10 Training Loss: 0.9603 Training Accuracy: 50.3672%\n",
      "Epoch: 9/10 Validation Loss: 0.9924 Validation Accuracy: 47.6500%\n",
      "Epoch: 10/10 Training Loss: 0.9597 Training Accuracy: 50.4878%\n",
      "Epoch: 10/10 Validation Loss: 0.9991 Validation Accuracy: 47.3500%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.09      0.16      3972\n",
      "           1       0.50      0.71      0.59      5937\n",
      "           2       0.30      0.40      0.34      2375\n",
      "\n",
      "    accuracy                           0.45     12284\n",
      "   macro avg       0.47      0.40      0.36     12284\n",
      "weighted avg       0.49      0.45      0.40     12284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR OWN CODE BELOW\n",
    "model_glove = FFTextClassifierWithEmbeddings(hidden_size, num_classes)\n",
    "train_nn(num_epochs, model_glove, train_loader, dev_loader)\n",
    "gold_labs, pred_labs = predict_nn(model_glove, test_loader)\n",
    "print(classification_report(gold_labs, pred_labs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c20d5-12c8-4820-89e2-e53c11b77b42",
   "metadata": {},
   "source": [
    "# 3. Improving the Neural Text Classifier (max. 22 marks)\n",
    "\n",
    "This section allows you some more free reign to experiment with the neural text classifier. Below, we list several to-dos that you can solve in your own way. Please make sure to label your notebook cells clearly so that it is obvious which to-do each cell corresponds to.\n",
    "\n",
    "**TO-DO 3a:** Consider the neural text classifier we have just implemented. It has a number of limitations that we could improve. Describe three limitations and how you could improve them. For each improvement you propose, provide a brief explanation (up to 1 paragraph) of how it works. **(9 marks)**\n",
    "\n",
    "WRITE YOUR ANSWER HERE\n",
    "\n",
    "**TO-DO 3b:** Implement your improvements and compute the performance of your method. Make sure to comment your code to show where each new step is implemented. Use the validation set for any tuning you decide to do. Present your results clearly. **(13 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e0d0d-ac93-4f7b-8169-c81f6b151eed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "94b2f5880aac8b17d6800754289172b1652d089b680c7940005440d5538bed6e"
  },
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
