{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba36903-ee71-4a62-9bb8-8e0270fdae64",
   "metadata": {},
   "source": [
    "# Advanced Text Analytics Lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d244c460-fa21-482a-a6e6-4216f8f74c24",
   "metadata": {},
   "source": [
    "This notebook is the second of two lab notebooks that you will submit as part of your assessment for the Advanced Data Analytics unit. The notebook contains three sections:\n",
    "1. **Introducing Transformers:** This section introduces the Transformers library from HuggingFace, showing you how to use it to obtain contextualised embeddings from pretrained transformer models.\n",
    "2. **Classification with transformers:** Here we show you how to construct a neural network classifier using Transformers, and give you the task of applying it to irony detection in tweets.\n",
    "3. **OPTIONAL: More on Transformers:** Some pointers to other materials if you want to learn more about transformers, e.g., if using them in your summer project. \n",
    "\n",
    "## Learning Outcomes\n",
    "\n",
    "These sections will contain tutorial-like instructions, as you have seen in previous text analytics labs. On completing these sections, the intended learning outcomes are that you will be able to...\n",
    "1. Use pretrained transformers to obtain contextualised word embeddings.\n",
    "1. Construct classifiers with pretrained transformers. \n",
    "1. Find documentation on pretrained models in the Transformers library.\n",
    "\n",
    "## Your Tasks\n",
    "\n",
    "Inside each of these sections there are several **'To-do's**, which you must complete for your summative assessment. Your marks will be based on your answers to these to-dos. Please make sure to:\n",
    "1. Include the output of your code in the saved notebook. Plots and printed output should be visible without re-running the code. \n",
    "1. Include all code needed to generate your answers.\n",
    "1. Provide sufficient comments to understand how your method works.\n",
    "1. Write text in a cell in markdown format where a written answer is required. You can convert a cell to markdown format by pressing Escape-M. \n",
    "\n",
    "There are also some unmarked 'to-do's that are part of the tutorial to help you learn how to implement and use the methods studied here.\n",
    "\n",
    "## Marking Criteria\n",
    "\n",
    "1. The coursework (both notebooks) is worth 30% of the unit in total. \n",
    "1. There is a total of 100 marks available for both lab notebooks. \n",
    "1. This notebook is worth 34 of those marks.\n",
    "1. The number of marks for each to-do out of 100 is shown alongside each to-do.\n",
    "1. For to-dos that require you to write code, a good solution would meet the following criteria (in order of importance):\n",
    "   1. Solves the task or answers the question asked in the to-do. This means, if the code cells in the notebook are executed in order, we will get the output shown in your notebook.\n",
    "   1. The code is easy to follow and does not contain unnecessary steps.\n",
    "   1. The comments show that you understand how your solution works.\n",
    "   1. A very good answer will also provide code that is computationally efficient but easy to read.\n",
    "1. You can use any suitable publicly available libraries. Unless the task explicitly asks you to implement something from scratch, there is no penalty for using libraries to implement some steps.\n",
    "\n",
    "## Support\n",
    "\n",
    "The main source of support will be during the remaining lab sessions (Fridays 2-5pm) for this unit. \n",
    "\n",
    "The TAs and lecturer will help you with questions about the lectures, the code provided for you in this notebook, and general questions about the topics we cover. For the marked 'to-dos', they can only answer clarifying questions about what you have to do. \n",
    "\n",
    "Office hours: You can book office hours with Edwin on Tuesdays 3pm-5pm by sending him an email (edwin.simpson@bristol.ac.uk). If those times are not possible for you, please contact him by email to request an alternative. \n",
    "\n",
    "## Deadline\n",
    "\n",
    "The notebook must be submitted along with the second notebook on Blackboard before **Wednesday 11th May at 13.00**. \n",
    "\n",
    "## Submission\n",
    "\n",
    "You will need to zip up this notebook with the previous notebook into a single .zip file, which you will submit to Blackboard through the 'assessment, submission and feedback' link on the left sidebar. \n",
    "\n",
    "Please name your files like this:\n",
    "   * Name this notebook ADA2_<student_number>.ipynb\n",
    "   * Name the zip file <student_number>.zip\n",
    "   * Please don't use your name anywhere as we want to mark anonymously. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c7d484-4669-467f-8906-1e7db2f8973d",
   "metadata": {},
   "source": [
    "# 0. Packages\n",
    "\n",
    "You will need to upgrade the version of transformers to transformers=4.14.1 . An older version was previously included by mistake in the crossplatform_environment.yml, but this is not needed. You can upgrade using:\n",
    "\n",
    "``conda upgrade -c huggingface transformers``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8a9045-9d0b-4427-b91c-a085496de2a5",
   "metadata": {},
   "source": [
    "# 4. Pretrained Transformers (max. 12 marks)\n",
    "\n",
    "HuggingFace is a company that has developed an open source library for loading pretrained transformer models. They also distribute many models themselves.  It is therefore the best library to use to create NLP models on top of large, deep neural networks. This is especially useful for tasks where simpler, feature-based methods or smaller LSTM models do not perform well enough, for example, when complex processing of syntax and semantics is required (natural language 'understanding'). \n",
    "\n",
    "Let's start by looking at two key types of object in the transformers library: models and tokenizers.\n",
    "\n",
    "## 4.1. Models\n",
    "\n",
    "The neural network models available in the Transformers library are accessed through wrapper classes such as `AutoModel`. If we want to load a pretrained model, we can simply pass its name to the `from_pretrained` function, and the pretrained model weights will be downloaded from HuggingFace and a neural network model will be created with those weights. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "122d993d-e48c-4dc2-bc14-2949b3d67e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at C:\\Users\\12055/.cache\\huggingface\\transformers\\3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at C:\\Users\\12055/.cache\\huggingface\\transformers\\1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at prajjwal1/bert-tiny.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel # For BERTs\n",
    "\n",
    "model = AutoModel.from_pretrained(\"prajjwal1/bert-tiny\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe226ec-580f-4022-97b0-e5010ddfb55e",
   "metadata": {},
   "source": [
    "This code loads the BERT-tiny model, which is a compressed version of BERT with 4.4 million parameters, compared to the standard version of BERT, called 'BERT-base', which has 110 million parameters. While BERT-tiny will not perform as well as larger models, we will use it for this notebook to save memory and computation costs. See [documentation here](https://huggingface.co/prajjwal1/bert-tiny).  \n",
    "\n",
    "The same functions can be used to load other models from HuggingFace's repository simply by changing the model's name. Take a look at [the Models page](https://huggingface.co/models) so see what there is on offer. Do you recognise any of the models' names?\n",
    "\n",
    "# 4.2. Tokenizers\n",
    "\n",
    "Before we can apply a model to some text, we need to a create Tokenizer object. In Transfomers, Tokenizer objects convert raw text to a sequence of numbers. First, the tokenizer actually performs tokenization, then it maps each token to its numerical ID. There are lots of different tokenizers that we can use to preprocess text. If we are loading a pretrained model, we will need to choose the tokenizer that corresponds to that model. \n",
    "\n",
    "We can load the right tokenizer as follows, in the same way we loaded the model itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d24395ef-aa4a-4d62-a6ea-aa97d86f42c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at C:\\Users\\12055/.cache\\huggingface\\transformers\\3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/vocab.txt from cache at C:\\Users\\12055/.cache\\huggingface\\transformers\\585ac1c3dedc6b808dd35e8770afafe10905d3e723a02617af749d39db780e09.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at C:\\Users\\12055/.cache\\huggingface\\transformers\\3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at C:\\Users\\12055/.cache\\huggingface\\transformers\\3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0ed12c-cb5a-48c5-b67f-7d17b0a79514",
   "metadata": {},
   "source": [
    "Let's see what the BERT-tiny tokenizer does to an example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "73e5852f-46b3-4e28-b952-780f472b6a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'transform', '##er', 'architecture', 'is', 'widely', 'used', 'in', 'nl', '##p', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The transformer architecture is widely used in NLP.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6807c966-188f-4a0b-b435-4282d5aa0201",
   "metadata": {},
   "source": [
    "Let's compare with the NLTK tokenizer we have seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ee63d96e-2441-4591-bff9-3a1bf6289828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'transformer', 'architecture', 'is', 'widely', 'used', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk_tokens = word_tokenize(sentence)\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f2123-34a6-4095-9981-2966fdadd7d8",
   "metadata": {},
   "source": [
    "While NLTK keeps whole words as tokens, the BERT tokenizer splits some words into sub-words. Splitting is applied to words with low frequency in the training set, such as 'transformer'. \n",
    "\n",
    "**TO-DO 4.2a:** What is the benefit of splitting words into sub-words? **(2 marks)**\n",
    "\n",
    "WRITE YOUR ANSWER HERE.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aacbbcf",
   "metadata": {},
   "source": [
    "1. To some extents, Sub-Word Tokenization alleviates OOV (out-of-vocabulary) problem. When a unknown word token out of the vocabulary appears, the sub word tokenizer can split this token into several fragments, and some of these fragments have probobility of existing in the vocabulary.  \n",
    "1. When the amount of words in corpus is massive, the sub-word tokenization can actually decrease the size of vocabulary, thus reduced computing resources\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe1f8a9-f38b-4dbf-a0ed-1c0e7991f9c1",
   "metadata": {},
   "source": [
    "It is important to use the right tokenizer with a pretrained model as each model was trained with text tokenized in a particular way. After tokenization, the Tokenizer object can also map the tokens to their IDs (indexes in the vocabulary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "950440ae-d58f-4675-8fb4-6114462331fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1996, 10938, 2121, 4294, 2003, 4235, 2109, 1999, 17953, 2361, 1012]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae17f77c-46f4-4ab0-9eee-858fd0b237f2",
   "metadata": {},
   "source": [
    "## 4.3. Contextualised Embeddings\n",
    "\n",
    "Now that we have a sequence of tokens, we are almost ready to process the sequence using the pretrained model. \n",
    "\n",
    "Our model takes as input a PyTorch `tensor` object. In PyTorch, `tensor` is a muli-dimensional matrix. Here, we need a two-dimensional matrix, where each row is a sequence of input tokens corresponding to a single sentence or document. Let's convert our list of IDs to a 2-D tensor with a single row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "07007371-2125-4fb0-8511-63ade9a2f796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1996, 10938,  2121,  4294,  2003,  4235,  2109,  1999, 17953,  2361,\n",
      "          1012]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "ids_tensor = torch.tensor([ids])\n",
    "\n",
    "print(ids_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508047c1-96fa-4839-9eb3-8e64e91c5919",
   "metadata": {},
   "source": [
    "Now we can process the sequence using our model. The model maps the sequence of input IDs to a sequence of output vectors, which are contextualised word embeddings. The hidden state values produced in the last hidden layer of the model are used as the contextualised embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "494c8335-7c8a-47c2-b450-a3b775f2262f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.6550,  0.3572, -1.8545,  ..., -2.2321, -2.4890,  0.8569],\n",
      "         [-0.7762,  0.7065, -0.4053,  ..., -1.0436, -1.4757,  1.0586],\n",
      "         [-0.0331,  0.0583, -0.5069,  ..., -3.0095, -0.8549,  0.6007],\n",
      "         ...,\n",
      "         [-0.1059,  0.2619,  0.2993,  ..., -0.9318, -2.3270,  1.5508],\n",
      "         [ 0.2457,  0.2863,  0.6015,  ..., -2.2550, -2.1556,  0.7440],\n",
      "         [ 0.6512, -0.0050,  0.4048,  ..., -1.6719, -2.0540,  0.0476]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.9995, -0.0462, -0.9893,  0.8689, -0.9952,  0.6976, -0.7959, -0.8617,\n",
      "         -0.0733,  0.0631, -0.2004, -0.0188,  0.0957,  0.9973,  0.3465, -0.5966,\n",
      "         -0.0231,  0.2026, -0.5178, -0.9471,  0.9236, -0.1754, -0.5852, -0.9344,\n",
      "         -0.9866, -0.0732, -0.9979,  0.8367,  0.6465,  0.0414,  0.0751,  0.0109,\n",
      "         -0.9963, -0.0876,  0.9558,  0.9860, -0.8553,  0.1024,  0.2728, -0.9718,\n",
      "          0.8817,  0.7812, -0.9781,  0.8534, -0.9879, -0.0432, -0.9243,  0.9781,\n",
      "          0.0589,  0.9604,  0.9951, -0.7930, -0.1010,  0.9998,  0.4897,  0.9581,\n",
      "         -0.8921, -0.3732,  0.8974, -0.8067,  0.0946,  0.6253,  0.8512,  0.7500,\n",
      "          0.9389, -0.9983,  0.0902, -0.2616,  0.7745,  0.5502,  0.9998, -0.0035,\n",
      "         -0.9685, -0.0289, -0.0050, -0.9889,  0.6671, -0.0353, -0.8759, -0.1471,\n",
      "         -0.8774,  0.1050, -0.8785, -0.9993,  0.9985, -0.3990,  0.6585, -0.9918,\n",
      "         -0.8411,  0.9567, -0.6456,  0.0018, -0.9945,  0.9735,  0.3081,  0.9290,\n",
      "         -0.8975, -0.9738, -0.9883, -0.9672, -0.8038,  0.9768, -0.9417,  0.3995,\n",
      "         -0.8751,  0.1182, -0.9928, -0.7911,  0.1377, -0.1895,  0.9922,  0.5050,\n",
      "         -0.3065,  0.9948, -0.9934,  0.1128,  0.7057,  0.9215, -0.1071, -0.9760,\n",
      "         -0.2978, -0.9856, -0.9533,  0.6290, -0.9895,  0.9924,  0.9407,  0.9272]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
      "tensor([[ 0.6550,  0.3572, -1.8545,  ..., -2.2321, -2.4890,  0.8569],\n",
      "        [-0.7762,  0.7065, -0.4053,  ..., -1.0436, -1.4757,  1.0586],\n",
      "        [-0.0331,  0.0583, -0.5069,  ..., -3.0095, -0.8549,  0.6007],\n",
      "        ...,\n",
      "        [-0.1059,  0.2619,  0.2993,  ..., -0.9318, -2.3270,  1.5508],\n",
      "        [ 0.2457,  0.2863,  0.6015,  ..., -2.2550, -2.1556,  0.7440],\n",
      "        [ 0.6512, -0.0050,  0.4048,  ..., -1.6719, -2.0540,  0.0476]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_outputs = model(ids_tensor)\n",
    "print(model_outputs)\n",
    "embeddings = model_outputs['last_hidden_state'][0]\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cee4181-7518-484f-993d-f7abffcae010",
   "metadata": {},
   "source": [
    "We can retrieve the embedding vector for \"transform\" like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b3df76db-f81e-4916-b31d-34a8341a20db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.7762389   0.7064995  -0.40526837 -1.0537281   0.5996382  -1.4787799\n",
      " -0.06114486  1.0198277  -0.2592845  -1.3763579   0.15379119  1.0128261\n",
      "  0.7475126  -0.17591816  2.0003283  -1.0197401  -0.7689709   0.0530691\n",
      "  0.130649    0.19979407 -1.0313494  -0.5410453   1.0834258   0.49249512\n",
      "  2.2506454   1.3008718  -0.16233611  0.22524881  0.7293377   0.37714246\n",
      "  0.07086009  0.39800274 -0.37489387 -0.18650481  0.5223738  -2.747382\n",
      " -0.5368228   0.35264593 -1.8976287  -0.35527697  0.07477656 -0.39572445\n",
      " -0.55447954  0.6223204   1.0455049  -2.1943057   0.40990472 -0.62277496\n",
      "  2.219217   -0.13648622  0.89714205  0.8076682   0.1879431  -0.01698842\n",
      "  0.5216419  -0.32894918  0.07476728 -1.1039577   1.2602047   3.4293036\n",
      " -0.91396147 -1.8800973  -0.08931094 -0.7966867   0.06266165  0.69099706\n",
      " -0.73700416 -0.23590541 -0.42857617 -0.68002903 -0.6193421   0.01592653\n",
      "  1.6605312   0.66483396 -1.6665554   2.1701148   0.797216   -0.5222837\n",
      "  0.6280762  -0.41740733 -0.11712596 -1.3964881  -0.4869622  -0.00932698\n",
      " -0.06558087 -1.1050274  -0.42741463 -0.29012507  2.4332054  -0.7398765\n",
      " -0.84649986  0.28248656 -1.8344125   1.5181535  -0.8839085   1.8521163\n",
      "  0.72055036 -0.650858   -0.8761084   1.837184    0.63478345 -0.8092329\n",
      "  0.06354895 -1.2577459   1.4600729  -2.0344603  -1.2294667  -0.03929216\n",
      "  0.08280444 -2.0118256  -1.2031775   0.38175374  0.02157758 -0.01493742\n",
      " -0.8913141   0.16130333  0.4589482  -1.7394044   0.3124296   0.59614754\n",
      "  0.8450614   1.4435817   1.072203    0.2779997  -4.0687737  -1.0435941\n",
      " -1.4757135   1.0586445 ]\n",
      "The BERT-tiny embeddings have 128 dimensions.\n"
     ]
    }
   ],
   "source": [
    "emb = embeddings[1]\n",
    "\n",
    "# convert it to a numpy array so we can perform various operations on it later on\n",
    "emb = emb.detach().numpy()\n",
    "\n",
    "print(emb)\n",
    "print(f'The BERT-tiny embeddings have {emb.shape[0]} dimensions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc1d779-8ec7-462a-895b-e58a88d9bb63",
   "metadata": {},
   "source": [
    "TO-DO 4.3a: Retrieve the embedding for \"architecture\" (this to-do will not be marked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "827ca93a-d80d-4655-a417-6c6c6d1e07f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR ANSWER HERE\n",
    "emb2 = embeddings[2]\n",
    "emb2 = emb2.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c79402-f6b7-4fc3-9f47-50fcffccbe68",
   "metadata": {},
   "source": [
    "Sentences and documents usually have varying lengths. So, to put multiple sentences into a single tensor, we need to pad the sequences up to a maximum length. Luckily, the tokenizer class takes care of this for us. When we pass in a list of sentences, the tokenizer creates a matrix, where each row is a sequence of the same length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c03abaed-6738-44ab-a7dd-7775743f99ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2016,  2441,  1996,  2338,  2000,  3931,  4261,  1998,  2211,\n",
      "          2000,  3191, 12575,  1012,   102,     0,     0,     0],\n",
      "        [  101,  2116,  8141,  2424,  1996,  2034,  2338,  1997,  1037,  6925,\n",
      "          1997,  2048,  3655,  2000,  2022, 16801,  1012,   102],\n",
      "        [  101,  1045,  2064,  2338,  9735,  2005,  1996,  4164,  2279,  2733,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1996,  2610,  2359,  2000,  2338,  2032,  2005,  4439,  2205,\n",
      "          3435,  1012,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  1045,  2064,  3914,  9735,  2005,  1996,  4164,  2279,  2733,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"She opened the book to page 37 and began to read aloud.\",\n",
    "    \"Many readers find the first book of A Tale of Two Cities to be confusing.\",\n",
    "    \"I can book tickets for the concert next week.\",\n",
    "    \"The police wanted to book him for driving too fast.\",\n",
    "    \"I can reserve tickets for the concert next week.\"\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")  \n",
    "\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59a44fc-218e-462d-86ef-fee203417bb6",
   "metadata": {},
   "source": [
    "`model_inputs` is a dictionary containing three objects. The `input_ids` are the list of token IDs in the input sequences. \n",
    "\n",
    "TO-DO 4.3b: What value do the special padding tokens have? (this to-do is unmarked):  \n",
    "0\n",
    "\n",
    "Notice that the input_ids all start with the same token ID, 101, even though they have different first words. They also have token ID 102 before the padding tokens. This is because the tokenizer inserts two special tokens, which are used in some applicaions of BERT. 101 is the '[CLS]' token, which is a dummy token whose embedding can be trained to represent the whole sequence. the [CLS] token's embedding can then be used as input to a text classifier to classify a sentence or document. Token 102 is '[SEP]', which can be used to separate multiple input sequences in a single example. This is needed in tasks where multiple pieces of text are provided as input, e.g., a to build a classifier that can determine whether two sentences contradict each other. \n",
    "\n",
    "The `attention_mask` records which tokens are special padding tokens and which are real tokens. Tokens with a 0 in the attention mask will be ignored.\n",
    "\n",
    "`token_type_ids` is needed when two sequences are passed together as input to the model. Here, each input is a single sentence, so we have only one type of token in the output above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b888b7cf-54d1-450b-9431-e64629177d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_inputs is a dictionary, so to provide the arguments to model(), \n",
    "# we use the double star to unpack the dictionary so that each key in the dictionary is\n",
    "# an argument to model() and each value is the value of the argument. \n",
    "model_outputs = model(**model_inputs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a8eb74-ce63-459c-9fb3-05fd2768ddf8",
   "metadata": {},
   "source": [
    "**TO-DO 4.3c:** The first four example sentences above all contain the word \"book\", and the last examples contains \"reserve\". Obtain the contextualised word embeddings for 'book' and 'reserve' in the example sentences using our model. **(4 marks)**\n",
    "\n",
    "Hint: you may need to convert tensors to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5f0297da-4106-41ef-8f61-5aa3fbb2cc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 128)\n"
     ]
    }
   ],
   "source": [
    "#WRITE YOUR OWN CODE HERE\n",
    "import numpy as np\n",
    "\n",
    "book_id = tokenizer.convert_tokens_to_ids(\"book\")\n",
    "reserve_id = tokenizer.convert_tokens_to_ids(\"reserve\")\n",
    "model_inputIDs_list = model_inputs[\"input_ids\"].detach().numpy()\n",
    "\n",
    "embedding_list = model_outputs[\"last_hidden_state\"]\n",
    "\n",
    "bookANDreserve_embeddings = []\n",
    "for idx, sent in enumerate(model_inputIDs_list):\n",
    "    if idx != 4:\n",
    "        # Find the location of the word \"book\" in the first sentence\n",
    "        loc = list(sent).index(book_id)\n",
    "    else:\n",
    "        # Find the location of the word \"reserve\" in the last sentence\n",
    "        loc = list(sent).index(reserve_id)\n",
    "    bookANDreserve_embeddings.append(embedding_list[idx][loc].detach().numpy())\n",
    "print(np.array(bookANDreserve_embeddings).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac47b72-46a3-40ee-b081-bca17da0b49b",
   "metadata": {},
   "source": [
    "**TO-DO 4.3d:** Write code to compare these embeddings in the cell below. In a few sentences, explain what your comparison tells us about the contextualised embeddings for \"book\". **(6 marks)**\n",
    "\n",
    "WRITE YOUR ANSWER HERE\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e3742",
   "metadata": {},
   "source": [
    "I made a similarity matrix for the words \"book\" and \"reserve\" in these five sentences based on euclidean distance. Following, I will illustrate the effectiveness of contextualised embedding based on this case.   \n",
    "  \n",
    "It can be seen from the matrix that the “book” word embeddings in the first two sentences are very close together, compared with other embeddings. And from the perspective of human understanding, \"book\" in the first two sentences both indicate the noun \"book\" that can be read.  \n",
    "  \n",
    "In the third sentence, the word “book” is similar in meaning to the word “reserve” in the fifth sentence. This is also reflected in the embedding space, where the word embedding of \"book\" in the third sentence and the word embedding of \"reserve\" in the fifth sentence are closer to each other than other shown word embeddings.   \n",
    "  \n",
    "As for \"book\" in the fourth sentence, its wordiness is a verb, the same as \"book\" in the third sentence, which may explain why the distance between the two word embeddings is shorter in the embedding space than other word embeddings.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c5a9c5c1-b6b2-4df4-8729-0f0615b5e161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   book sent 1  book sent 2  book sent 3  book sent 4  reserve sent 5\n",
      "0     0.000000     8.520038     9.546872    10.407844       13.018836\n",
      "1     8.520038     0.000000    10.206396    10.488382       13.664334\n",
      "2     9.546872    10.206396     0.000000     9.426083        7.451720\n",
      "3    10.407844    10.488382     9.426083     0.000000       11.671614\n",
      "4    13.018836    13.664334     7.451720    11.671614        0.000000\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR ANSWER HERE\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "dists = np.empty((5,5))\n",
    "for row, word_emb_row in enumerate(bookANDreserve_embeddings):\n",
    "    for col, word_emb_col in enumerate(bookANDreserve_embeddings):\n",
    "        dists[row][col] = cdist([word_emb_row], [word_emb_col], 'euclidean')\n",
    "row_name = [\"book sent 1\",\"book sent 2\",\"book sent 3\",\"book sent 4\",\"reserve sent 5\"]\n",
    "sim_mat = pd.DataFrame(data = dists, columns= row_name)\n",
    "print(sim_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768029d-395e-4eb9-ac1b-7c72e81dcd01",
   "metadata": {},
   "source": [
    "# 5. Transformer-basd Text Classifiers (max. 22 marks)\n",
    "\n",
    "The previous section showed us how to obtain a sequence of contextualised word embeddings using a pretrained transformer. How can we use a pretrained model to build a classifier?\n",
    "\n",
    "First, let's load up the [Tweet Eval](https://huggingface.co/datasets/tweet_eval) emotion analysis dataset, which we will use to train and test a classifier. The Emotion dataset is relatively small compared to the sentiment dataset we used earlier. The task is to classify tweets into one of  0: anger, 1: joy, 2: optimism, or 3: sadness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5fcd5efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache\\tweet_eval\\emotion\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with 3257 instances loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache\\tweet_eval\\emotion\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset with 374 instances loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache\\tweet_eval\\emotion\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset with 1421 instances loaded\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"emotion\",\n",
    "    split=\"train\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")\n",
    "\n",
    "val_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"emotion\",\n",
    "    split=\"validation\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Validation dataset with {len(val_dataset)} instances loaded\")\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"emotion\",\n",
    "    split=\"test\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")\n",
    "num_classes = np.unique(train_dataset['label']).size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f2f2c-8f39-49b0-8391-33773171c5a9",
   "metadata": {},
   "source": [
    "Now we are working with a proper dataset, which uses the datasets library. We can use our tokenizer to tokenize the examples in the dataset using the code in the next cell. Here, we use the ``map()`` method again to apply the tokenizer to each example in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d5b94acc-77ff-434e-99d0-ec49f00fa58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at ./data_cache\\tweet_eval\\emotion\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343\\cache-c34cae45b308f8b3.arrow\n",
      "Loading cached processed dataset at ./data_cache\\tweet_eval\\emotion\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343\\cache-dd50e62ebbf10e21.arrow\n",
      "Loading cached processed dataset at ./data_cache\\tweet_eval\\emotion\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343\\cache-5880ad1ede6e3d2a.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(dataset):\n",
    "    model_inputs = tokenizer(dataset['text'], padding=\"max_length\", max_length=100, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510ccaf0-fc74-405b-b72f-f03328d6622a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, we have the dataset in the right format, let's see how to create a classifier based on a pretrained transformer.\n",
    "\n",
    "Our original text classifier from the first notebook used a fully-connected layer to produce a hidden representation of the whole sentence. This hidden representation was then fed to an output layer to produce a probability distribution over class labels:\n",
    "\n",
    "<img src=\"neural_text_classifier_smaller.png\" alt=\"Neural text classifier diagram from the slides in lecture 8.1\" width=\"400px\"/>\n",
    "\n",
    "With transformers, we can do something very similar, by connecting the transfomer's output to a fully-connected layer. However, with BERT, we do not need to pass the embedding of each individual word to the fully-connected layer because there is a special [CLS] token that represents the whole sentence:\n",
    "\n",
    "<img src=\"bert_text_classifier.png\" alt=\"BERT text classifier diagram from the slides in lecture 9.2\" width=\"400px\"/>\n",
    "\n",
    "Diagram from [\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"](https://teaching.bb-ai.net/Student-Projects/Winograd-Challenge-Papers/2018-Devlin-BERT.pdf), Devlin et al., 2018.\n",
    "\n",
    "The code below shows how to access a tensor containing the [CLS] embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "32808677-1efb-49aa-a384-cd8ca1632d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "cls_embs = model(**model_inputs)['last_hidden_state'][:, 0]\n",
    "\n",
    "print(cls_embs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dff251c-ccd2-4e0c-ab2b-a1bd1cc38101",
   "metadata": {},
   "source": [
    "So, given the pretrained BERT model, we need to put a classifier 'head' (fully connected layers that map the CLS embedding to a class probability) onto it, and train the classifier head for emotion classification. \n",
    "\n",
    "The transformers library provides some useful wrappers around the pretrained models that construct complete models for typical tasks such as text classification. These auto classes are documented here: https://huggingface.co/docs/transformers/model_doc/auto\n",
    "\n",
    "The code below will create a complete model for sequence classification, based on the BERT-tiny model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "99068090-6a2d-47f0-9d22-b78a6705cd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at C:\\Users\\12055/.cache\\huggingface\\transformers\\3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at C:\\Users\\12055/.cache\\huggingface\\transformers\\1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\", num_labels=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e96b27d-0071-4d97-9d99-839f35b6d79c",
   "metadata": {},
   "source": [
    "**TO-DO 5a:** Can you use this model straight away to classify emotions? Explain your answer. **(2 marks)**\n",
    "\n",
    "WRITE YOUR ANSWER HERE\n",
    "\n",
    "Yes. In order to directly classify with the BERT model, we need to fine-tune the BERT model. On a BERTmodel, we can retrain model with the target dataset, which is much smaller than the dataset for pretraining. Then, through backpropagation, the pre-trained weights of the model are updated based on the new dataset (of course, we also can freeze all layers except the final layer). When predicting categories, the model would feed the output to the softmax layer, and the predicted results would be stored in the first token [ELC] of the final layer.  \n",
    "\n",
    "**TO-DO 5b:** If you want to perform NER, you would need to replace the use of `AutoModelForSequenceClassification` in the cell above to load a suitable model. Which auto class could you use, and how would the model differ from the model we have loaded above? Reference the documentation for the chosen class in your answer. **(4 marks)** \n",
    "\n",
    "WRITE YOUR ANSWER HERE  \n",
    "AutoModelForTokenClassification  \n",
    "From the names, we can understand that AutoModelForTokenClassification is a generic model class that can instantiate the token classification model, and AutoModelForSequenceClassification is for instantiating the sequence classification model. I think the biggest difference between them is the granularity of output embeddings, one is word embedding and another is sequence embedding.  [Document](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelfortokenclassification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a38d4e-27a1-4c0c-b3fa-e8dc3bdf61b5",
   "metadata": {},
   "source": [
    "Next, we are going to train our model. Sometimes it is not necessary to update the weights in the BERT model itself, so we can freeze them. This can save a lot of computation time. We can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f8ad56b1-d6e7-4048-9cc0-71849a003363",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc27e2c-c1d4-4692-9386-f30e477293bd",
   "metadata": {},
   "source": [
    "To train our model on the emotion data, we can make use of the Trainer class. This class encapsulates a lot of the complex training steps and avoids the need to define our own training function (``train_nn`` in the previous notebook).\n",
    "\n",
    "Run the code below to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "62af0a20-79a5-49ca-882a-638a68b288f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"transformer_checkpoints\",  # specify the directory where models weights will be saved a certain points during training (checkpoints)\n",
    "    num_train_epochs=3,  # change this if it is taking too long on your computer\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9e5ba93c-f8c4-4542-a316-6aac4a32362d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\12055\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3257\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1224\n",
      " 41%|████      | 500/1224 [00:16<00:20, 35.84it/s]Saving model checkpoint to transformer_checkpoints\\checkpoint-500\n",
      "Configuration saved in transformer_checkpoints\\checkpoint-500\\config.json\n",
      "Model weights saved in transformer_checkpoints\\checkpoint-500\\pytorch_model.bin\n",
      " 41%|████▏     | 506/1224 [00:16<00:21, 33.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3026, 'learning_rate': 2.957516339869281e-05, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 1000/1224 [00:31<00:08, 25.21it/s]Saving model checkpoint to transformer_checkpoints\\checkpoint-1000\n",
      "Configuration saved in transformer_checkpoints\\checkpoint-1000\\config.json\n",
      "Model weights saved in transformer_checkpoints\\checkpoint-1000\\pytorch_model.bin\n",
      " 82%|████████▏ | 1003/1224 [00:31<00:09, 23.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.262, 'learning_rate': 9.150326797385621e-06, 'epoch': 2.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1223/1224 [00:39<00:00, 29.12it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 1224/1224 [00:39<00:00, 31.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 39.2278, 'train_samples_per_second': 249.083, 'train_steps_per_second': 31.202, 'train_loss': 1.2793277167027293, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1224, training_loss=1.2793277167027293, metrics={'train_runtime': 39.2278, 'train_samples_per_second': 249.083, 'train_steps_per_second': 31.202, 'train_loss': 1.2793277167027293, 'epoch': 3.0})"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccea6f4d-23fa-4b34-9eef-35762ef0e831",
   "metadata": {},
   "source": [
    "Let's make some predictions with our model on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ad4e689d-ebec-44c4-b81d-649079e4a4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn(trained_model, test_dataset):\n",
    "\n",
    "    # Pass the required items from the dataset to the model    \n",
    "    output = trained_model(attention_mask=torch.tensor(test_dataset[\"attention_mask\"]), input_ids=torch.tensor(test_dataset[\"input_ids\"]))\n",
    "        \n",
    "    # the output dictionary contains logits, which are the unnormalised scores for each class for each example:\n",
    "    pred_labs = np.argmax(output[\"logits\"].detach().numpy(), axis=1)\n",
    "    \n",
    "    gold_labs = test_dataset[\"label\"]\n",
    "    \n",
    "    return gold_labs, pred_labs\n",
    "\n",
    "# Run the prediction function to get the results:\n",
    "gold_labs, pred_labs = predict_nn(model, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddf8564-4f98-4e3f-b1f7-b59725a0605c",
   "metadata": {},
   "source": [
    "**TO-DO 5c:** \n",
    "Implement and test a classifier for the \"irony\" subset of the [Tweet_eval dataset](https://huggingface.co/datasets/tweet_eval) using a pretrained transformer. Evaluate the classifier with both frozen and unfrozen (i.e., fine-tuned) BERT layers. Choose a suitable evaluation metric and provide a comparison of the results below, including a brief explanation (1-2 sentences) for any differences you observe. Make sure to comment your code.  **(8 marks)**\n",
    "\n",
    "Note: you may implement any kind of classifier you like, as long as you are using a pretrained transformer model. \n",
    "\n",
    "WRITE YOUR ANSWER HERE   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "62aeb113-2262-4be4-b318-3791775734c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache\\tweet_eval\\irony\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with 2862 instances loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache\\tweet_eval\\irony\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset with 955 instances loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache\\tweet_eval\\irony\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "Loading cached processed dataset at ./data_cache\\tweet_eval\\irony\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343\\cache-0a3f795c693ba54c.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset with 784 instances loaded\n",
      "training dataset{1: 1445, 0: 1417}\n",
      "validation dataset{1: 456, 0: 499}\n",
      "test dataset{0: 473, 1: 311}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 15.87ba/s]\n",
      "Loading cached processed dataset at ./data_cache\\tweet_eval\\irony\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343\\cache-19de43ce85521bc7.arrow\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR ANSWER HERE\n",
    "# Loda the train and test dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"irony\",\n",
    "    split=\"train\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")\n",
    "\n",
    "val_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"irony\",\n",
    "    split=\"validation\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Validation dataset with {len(val_dataset)} instances loaded\")\n",
    "test_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"irony\",\n",
    "    split=\"test\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")\n",
    "num_classes = np.unique(train_dataset['label']).size\n",
    "\n",
    "print(f\"training dataset{dict(Counter(train_dataset['label']))}\")\n",
    "print(f\"validation dataset{dict(Counter(val_dataset['label']))}\")\n",
    "print(f\"test dataset{dict(Counter(test_dataset['label']))}\")\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "70021431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/vinai/bertweet-base/resolve/main/config.json not found in cache or force_download set to True, downloading to C:\\Users\\12055\\.cache\\huggingface\\transformers\\tmpsaxvx8v0\n",
      "Downloading: 100%|██████████| 558/558 [00:00<00:00, 558kB/s]\n",
      "storing https://huggingface.co/vinai/bertweet-base/resolve/main/config.json in cache at C:\\Users\\12055/.cache\\huggingface\\transformers\\356366feedcea0917e30f7f235e1e062ffc2d28138445d5672a184be756c8686.a2b6026e688d1b19cebc0981d8f3a5b1668eabfda55b2c42049d5eac0bc8cb2d\n",
      "creating metadata file for C:\\Users\\12055/.cache\\huggingface\\transformers\\356366feedcea0917e30f7f235e1e062ffc2d28138445d5672a184be756c8686.a2b6026e688d1b19cebc0981d8f3a5b1668eabfda55b2c42049d5eac0bc8cb2d\n",
      "loading configuration file https://huggingface.co/vinai/bertweet-base/resolve/main/config.json from cache at C:\\Users\\12055/.cache\\huggingface\\transformers\\356366feedcea0917e30f7f235e1e062ffc2d28138445d5672a184be756c8686.a2b6026e688d1b19cebc0981d8f3a5b1668eabfda55b2c42049d5eac0bc8cb2d\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "https://huggingface.co/vinai/bertweet-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to C:\\Users\\12055\\.cache\\huggingface\\transformers\\tmphqoje0k4\n",
      "Downloading: 100%|██████████| 517M/517M [04:22<00:00, 2.07MB/s] \n",
      "storing https://huggingface.co/vinai/bertweet-base/resolve/main/pytorch_model.bin in cache at C:\\Users\\12055/.cache\\huggingface\\transformers\\4e07e2989cb95a6f63c704a7170b48e6e663cc203c05db424e47f4d75562cf0e.7b2adda243ecb4b085eb2d22ef1b2cd12a882a43bbb13a34c11e10f960b9bfc3\n",
      "creating metadata file for C:\\Users\\12055/.cache\\huggingface\\transformers\\4e07e2989cb95a6f63c704a7170b48e6e663cc203c05db424e47f4d75562cf0e.7b2adda243ecb4b085eb2d22ef1b2cd12a882a43bbb13a34c11e10f960b9bfc3\n",
      "loading weights file https://huggingface.co/vinai/bertweet-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\12055/.cache\\huggingface\\transformers\\4e07e2989cb95a6f63c704a7170b48e6e663cc203c05db424e47f4d75562cf0e.7b2adda243ecb4b085eb2d22ef1b2cd12a882a43bbb13a34c11e10f960b9bfc3\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/vinai/bertweet-base/resolve/main/config.json from cache at C:\\Users\\12055/.cache\\huggingface\\transformers\\356366feedcea0917e30f7f235e1e062ffc2d28138445d5672a184be756c8686.a2b6026e688d1b19cebc0981d8f3a5b1668eabfda55b2c42049d5eac0bc8cb2d\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/vinai/bertweet-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\12055/.cache\\huggingface\\transformers\\4e07e2989cb95a6f63c704a7170b48e6e663cc203c05db424e47f4d75562cf0e.7b2adda243ecb4b085eb2d22ef1b2cd12a882a43bbb13a34c11e10f960b9bfc3\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RobertaForSequenceClassification' object has no attribute 'bert'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\UoB\\Advanced-Data-Dnalytics\\week10\\Week 10 Lab -- Transformers\\ADA2.ipynb Cell 51'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/UoB/Advanced-Data-Dnalytics/week10/Week%2010%20Lab%20--%20Transformers/ADA2.ipynb#ch0000059?line=0'>1</a>\u001b[0m model_forzen \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mvinai/bertweet-base\u001b[39m\u001b[39m\"\u001b[39m, num_labels\u001b[39m=\u001b[39mnum_classes)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/UoB/Advanced-Data-Dnalytics/week10/Week%2010%20Lab%20--%20Transformers/ADA2.ipynb#ch0000059?line=1'>2</a>\u001b[0m model_tuneAll \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mvinai/bertweet-base\u001b[39m\u001b[39m\"\u001b[39m, num_labels\u001b[39m=\u001b[39mnum_classes)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/UoB/Advanced-Data-Dnalytics/week10/Week%2010%20Lab%20--%20Transformers/ADA2.ipynb#ch0000059?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m model_forzen\u001b[39m.\u001b[39;49mbert\u001b[39m.\u001b[39mparameters():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/UoB/Advanced-Data-Dnalytics/week10/Week%2010%20Lab%20--%20Transformers/ADA2.ipynb#ch0000059?line=3'>4</a>\u001b[0m     param\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/UoB/Advanced-Data-Dnalytics/week10/Week%2010%20Lab%20--%20Transformers/ADA2.ipynb#ch0000059?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_trainer\u001b[39m(model):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/12055/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1182'>1183</a>\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   <a href='file:///c%3A/Users/12055/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1183'>1184</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> <a href='file:///c%3A/Users/12055/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1184'>1185</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   <a href='file:///c%3A/Users/12055/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1185'>1186</a>\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RobertaForSequenceClassification' object has no attribute 'bert'"
     ]
    }
   ],
   "source": [
    "model_forzen = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels=num_classes)\n",
    "model_tuneAll = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels=num_classes)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", )\n",
    "for param in model_forzen.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "def get_trainer(model):\n",
    "    trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset)\n",
    "    return trainer\n",
    "trainer_forzen = get_trainer(model_forzen)\n",
    "trainer_tuneAll = get_trainer(model_tuneAll)\n",
    "trainer_forzen.train()\n",
    "trainer_tuneAll.train()\n",
    "gold_labs_forzen, pred_labs_forzen = predict_nn(model_forzen, test_dataset)\n",
    "gold_labs_tuneAll, pred_labs_tuneAll = predict_nn(model_tuneAll, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34f1245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for classifier with frozen\n",
      "0.4954128440366973\n",
      "F1 Score for classifier with unfrozen\n",
      "0.5608308605341247\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score for classifier with frozen\")\n",
    "print(f1_score(gold_labs_forzen, pred_labs_forzen))\n",
    "\n",
    "print(\"F1 Score for classifier with unfrozen\")\n",
    "print(f1_score(gold_labs_tuneAll, pred_labs_tuneAll))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f1661d-a203-42ee-8681-025f2bdfe661",
   "metadata": {},
   "source": [
    "**TO-DO 5d:** Briefly describe how the classifiers you implemented for the irony dataset use transfer learning. **(4 marks)**\n",
    "\n",
    "WRITE YOUR ANSWER HERE\n",
    "\n",
    "\n",
    "**TO-DO 5e:** Use your model to compute the probability of irony for a sentence of your choosing. Comment your code and print the sentence with its probability. **(4 marks)**\n",
    "\n",
    "Hint: you could choose a sentence from [this page on verbal irony](https://examples.yourdictionary.com/examples-of-verbal-irony.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fcdc69-ce30-4eb3-afc5-4eeba41b40db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR ANSWER HERE   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb0d80-4624-4690-a94d-b9f3c1a60d7f",
   "metadata": {},
   "source": [
    "# 6. OPTIONAL: More on Transformers\n",
    "\n",
    "There are many great resources out there to show you how to use this kind of model in practice:\n",
    "* An extensive online course is provided by HuggingFace: https://huggingface.co/course/chapter1/1. The pages linked from the HuggingFace course website have an 'open in Colab' button on the top right. You can open the notebook and run it on a Google server there to access GPUs.\n",
    "* Chapters that may be particularly useful: \n",
    "   * Transformers, what can they do? https://huggingface.co/course/chapter1/3?fw=pt\n",
    "   * Using Transformers: https://huggingface.co/course/chapter2/2?fw=pt\n",
    "* They provide information on fine-tuning the transformer models here: https://huggingface.co/docs/transformers/training. Fine-tuning updates the weights inside the pretrained network and requires extensive GPU or TPU computing. \n",
    "* Text Generation: https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb. This topic goes way beyond data analytics on this unit and shows you another powerful feature of pretrained transformers.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db6dbd6af82c8a65067133132fa7cc8afd2d273c6e54b5cae29f2d55acec71c4"
  },
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
